{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919308e7-c5c6-4e3e-915f-d85f653aac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "os.environ['http_proxy']=\"http://proxy.corp.qihoo.net:8080\"\n",
    "os.environ['OPENAI_API_KEY']=\"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c5fe8c-23e9-4a36-be4a-1ab3d0bb0cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"./state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50750f4d-d3b6-4aa3-b0b8-968aa57c860d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c96866a-5856-4d56-96b2-7f5e3779afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将索引暴露在一个检索器的接口\n",
    "retriever = docsearch.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dd345c3-444c-499c-b221-67bf580d210c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The president said that Ketanji Brown Jackson is one of our nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He also said she is a consensus builder and has received a broad range of support from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edadac5-8a82-4c84-aa26-014889ba58c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c737cb4-62f7-400c-8486-54112b01ba68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "347585fc-9922-401e-81f4-6e5dbf9e254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yhk-home/github/Chat_app'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8c605-b273-4ac0-b279-e6010dc97b3b",
   "metadata": {},
   "source": [
    "# 私人助手\n",
    "三个特点：  \n",
    "1. 能够与外界进行交流（Agent）\n",
    "2. 由私人数据相关知识（Index）\n",
    "3. 记得与你的互动(Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28210705-8db4-48d6-a12c-a7a82df1d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import deque\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "from langchain import LLMChain, OpenAI, PromptTemplate\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import BaseLLM\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains.base import Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d8a2c-632c-409d-be14-f57e1fde629a",
   "metadata": {},
   "source": [
    "### 连接到向量存储器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3eb140c-e424-4e7f-a03c-b96164261de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7cc190-f6ca-4fdd-979a-9de9f125d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义embedding模型\n",
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91c935dd-3b27-448b-9bb5-6bc7f82bf5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化向量存储\n",
    "import faiss\n",
    "embedding_size = 1536\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc53480-b59d-4d5a-9348-7ffc603c1be4",
   "metadata": {},
   "source": [
    "### 定义链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4451c40a-669d-4d5c-bc75-b984ede9d6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskCreationChain(LLMChain):\n",
    "    \"\"\"Chain to generates tasks.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n",
    "        \"\"\"Get the response parser.\"\"\"\n",
    "        task_creation_template = (\n",
    "            \"You are an task creation AI that uses the result of an execution agent\"\n",
    "            \" to create new tasks with the following objective: {objective},\"\n",
    "            \" The last completed task has the result: {result}.\"\n",
    "            \" This result was based on this task description: {task_description}.\"\n",
    "            \" These are incomplete tasks: {incomplete_tasks}.\"\n",
    "            \" Based on the result, create new tasks to be completed\"\n",
    "            \" by the AI system that do not overlap with incomplete tasks.\"\n",
    "            \" Return the tasks as an array.\"\n",
    "        )\n",
    "        prompt = PromptTemplate(\n",
    "            template=task_creation_template,\n",
    "            input_variables=[\"result\", \"task_description\", \"incomplete_tasks\", \"objective\"],\n",
    "        )\n",
    "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cf65ab0-bee3-4312-bf51-4c1e821bea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskPrioritizationChain(LLMChain):\n",
    "    \"\"\"Chain to prioritize tasks.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n",
    "        \"\"\"Get the response parser.\"\"\"\n",
    "        task_prioritization_template = (\n",
    "            \"You are an task prioritization AI tasked with cleaning the formatting of and reprioritizing\"\n",
    "            \" the following tasks: {task_names}.\"\n",
    "            \" Consider the ultimate objective of your team: {objective}.\"\n",
    "            \" Do not remove any tasks. Return the result as a numbered list, like:\"\n",
    "            \" #. First task\"\n",
    "            \" #. Second task\"\n",
    "            \" Start the task list with number {next_task_id}.\"\n",
    "        )\n",
    "        prompt = PromptTemplate(\n",
    "            template=task_prioritization_template,\n",
    "            input_variables=[\"task_names\", \"next_task_id\", \"objective\"],\n",
    "        )\n",
    "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "603a896c-9339-4370-a564-6c5703bc8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExecutionChain(LLMChain):\n",
    "    \"\"\"Chain to execute tasks.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n",
    "        \"\"\"Get the response parser.\"\"\"\n",
    "        execution_template = (\n",
    "            \"You are an AI who performs one task based on the following objective: {objective}.\"\n",
    "            \" Take into account these previously completed tasks: {context}.\"\n",
    "            \" Your task: {task}.\"\n",
    "            \" Response:\"\n",
    "        )\n",
    "        prompt = PromptTemplate(\n",
    "            template=execution_template,\n",
    "            input_variables=[\"objective\", \"context\", \"task\"],\n",
    "        )\n",
    "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c9c9ec-b2de-4a00-917a-d36c863c4728",
   "metadata": {},
   "source": [
    "### 定义控制器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20dc1117-660d-4411-94ce-cf9b6d2cd892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_task(task_creation_chain: LLMChain, result: Dict, task_description: str, task_list: List[str], objective: str) -> List[Dict]:\n",
    "    \"\"\"Get the next task.\"\"\"\n",
    "    incomplete_tasks = \", \".join(task_list)\n",
    "    response = task_creation_chain.run(result=result, task_description=task_description, incomplete_tasks=incomplete_tasks, objective=objective)\n",
    "    new_tasks = response.split('\\n')\n",
    "    return [{\"task_name\": task_name} for task_name in new_tasks if task_name.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e98b92eb-a571-4e8d-8fc8-d0f5aea6e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritize_tasks(task_prioritization_chain: LLMChain, this_task_id: int, task_list: List[Dict], objective: str) -> List[Dict]:\n",
    "    \"\"\"Prioritize tasks.\"\"\"\n",
    "    task_names = [t[\"task_name\"] for t in task_list]\n",
    "    next_task_id = int(this_task_id) + 1\n",
    "    response = task_prioritization_chain.run(task_names=task_names, next_task_id=next_task_id, objective=objective)\n",
    "    new_tasks = response.split('\\n')\n",
    "    prioritized_task_list = []\n",
    "    for task_string in new_tasks:\n",
    "        if not task_string.strip():\n",
    "            continue\n",
    "        task_parts = task_string.strip().split(\".\", 1)\n",
    "        if len(task_parts) == 2:\n",
    "            task_id = task_parts[0].strip()\n",
    "            task_name = task_parts[1].strip()\n",
    "            prioritized_task_list.append({\"task_id\": task_id, \"task_name\": task_name})\n",
    "    return prioritized_task_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5eeb71fe-4753-4dc0-9522-5df548e72402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_top_tasks(vectorstore, query: str, k: int) -> List[str]:\n",
    "    \"\"\"Get the top k tasks based on the query.\"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    if not results:\n",
    "        return []\n",
    "    sorted_results, _ = zip(*sorted(results, key=lambda x: x[1], reverse=True))\n",
    "    return [str(item.metadata['task']) for item in sorted_results]\n",
    "\n",
    "def execute_task(vectorstore, execution_chain: LLMChain, objective: str, task: str, k: int = 5) -> str:\n",
    "    \"\"\"Execute a task.\"\"\"\n",
    "    context = _get_top_tasks(vectorstore, query=objective, k=k)\n",
    "    return execution_chain.run(objective=objective, context=context, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db27583d-456f-410d-9440-15db69027fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyAGI(Chain, BaseModel):\n",
    "    \"\"\"Controller model for the BabyAGI agent.\"\"\"\n",
    "\n",
    "    task_list: deque = Field(default_factory=deque)\n",
    "    task_creation_chain: TaskCreationChain = Field(...)\n",
    "    task_prioritization_chain: TaskPrioritizationChain = Field(...)\n",
    "    execution_chain: ExecutionChain = Field(...)\n",
    "    task_id_counter: int = Field(1)\n",
    "    vectorstore: VectorStore = Field(init=False)\n",
    "    max_iterations: Optional[int] = None\n",
    "        \n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def add_task(self, task: Dict):\n",
    "        self.task_list.append(task)\n",
    "\n",
    "    def print_task_list(self):\n",
    "        print(\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\\033[0m\")\n",
    "        for t in self.task_list:\n",
    "            print(str(t[\"task_id\"]) + \": \" + t[\"task_name\"])\n",
    "\n",
    "    def print_next_task(self, task: Dict):\n",
    "        print(\"\\033[92m\\033[1m\" + \"\\n*****NEXT TASK*****\\n\" + \"\\033[0m\\033[0m\")\n",
    "        print(str(task[\"task_id\"]) + \": \" + task[\"task_name\"])\n",
    "\n",
    "    def print_task_result(self, result: str):\n",
    "        print(\"\\033[93m\\033[1m\" + \"\\n*****TASK RESULT*****\\n\" + \"\\033[0m\\033[0m\")\n",
    "        print(result)\n",
    "        \n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return [\"objective\"]\n",
    "    \n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return []\n",
    "\n",
    "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Run the agent.\"\"\"\n",
    "        objective = inputs['objective']\n",
    "        first_task = inputs.get(\"first_task\", \"Make a todo list\")\n",
    "        self.add_task({\"task_id\": 1, \"task_name\": first_task})\n",
    "        num_iters = 0\n",
    "        while True:\n",
    "            if self.task_list:\n",
    "                self.print_task_list()\n",
    "\n",
    "                # Step 1: Pull the first task\n",
    "                task = self.task_list.popleft()\n",
    "                self.print_next_task(task)\n",
    "\n",
    "                # Step 2: Execute the task\n",
    "                result = execute_task(\n",
    "                    self.vectorstore, self.execution_chain, objective, task[\"task_name\"]\n",
    "                )\n",
    "                this_task_id = int(task[\"task_id\"])\n",
    "                self.print_task_result(result)\n",
    "\n",
    "                # Step 3: Store the result in Pinecone\n",
    "                result_id = f\"result_{task['task_id']}\"\n",
    "                self.vectorstore.add_texts(\n",
    "                    texts=[result],\n",
    "                    metadatas=[{\"task\": task[\"task_name\"]}],\n",
    "                    ids=[result_id],\n",
    "                )\n",
    "\n",
    "                # Step 4: Create new tasks and reprioritize task list\n",
    "                new_tasks = get_next_task(\n",
    "                    self.task_creation_chain, result, task[\"task_name\"], [t[\"task_name\"] for t in self.task_list], objective\n",
    "                )\n",
    "                for new_task in new_tasks:\n",
    "                    self.task_id_counter += 1\n",
    "                    new_task.update({\"task_id\": self.task_id_counter})\n",
    "                    self.add_task(new_task)\n",
    "                self.task_list = deque(\n",
    "                    prioritize_tasks(\n",
    "                        self.task_prioritization_chain, this_task_id, list(self.task_list), objective\n",
    "                    )\n",
    "                )\n",
    "            num_iters += 1\n",
    "            if self.max_iterations is not None and num_iters == self.max_iterations:\n",
    "                print(\"\\033[91m\\033[1m\" + \"\\n*****TASK ENDING*****\\n\" + \"\\033[0m\\033[0m\")\n",
    "                break\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLLM,\n",
    "        vectorstore: VectorStore,\n",
    "        verbose: bool = False,\n",
    "        **kwargs\n",
    "    ) -> \"BabyAGI\":\n",
    "        \"\"\"Initialize the BabyAGI Controller.\"\"\"\n",
    "        task_creation_chain = TaskCreationChain.from_llm(\n",
    "            llm, verbose=verbose\n",
    "        )\n",
    "        task_prioritization_chain = TaskPrioritizationChain.from_llm(\n",
    "            llm, verbose=verbose\n",
    "        )\n",
    "        execution_chain = ExecutionChain.from_llm(llm, verbose=verbose)\n",
    "        return cls(\n",
    "            task_creation_chain=task_creation_chain,\n",
    "            task_prioritization_chain=task_prioritization_chain,\n",
    "            execution_chain=execution_chain,\n",
    "            vectorstore=vectorstore,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c79a4-4143-4a87-ac97-467964470697",
   "metadata": {},
   "source": [
    "### 运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f3637fb-4b52-4a20-9cb7-ea291bf9b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECTIVE = \"Write a weather report for SF today\"\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd73df10-6301-4992-b5f1-5791cb87e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging of LLMChains\n",
    "verbose=False\n",
    "# If None, will keep on going forever\n",
    "max_iterations: Optional[int] = 3\n",
    "baby_agi = BabyAGI.from_llm(\n",
    "    llm=llm,\n",
    "    vectorstore=vectorstore,\n",
    "    verbose=verbose,\n",
    "    max_iterations=max_iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "196bcaad-e78e-459c-8074-2109efff161e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[1m\n",
      "*****TASK LIST*****\n",
      "\u001b[0m\u001b[0m\n",
      "1: Make a todo list\n",
      "\u001b[92m\u001b[1m\n",
      "*****NEXT TASK*****\n",
      "\u001b[0m\u001b[0m\n",
      "1: Make a todo list\n",
      "\u001b[93m\u001b[1m\n",
      "*****TASK RESULT*****\n",
      "\u001b[0m\u001b[0m\n",
      "\n",
      "\n",
      "1. Check the current temperature in San Francisco\n",
      "2. Check the forecast for the day\n",
      "3. Check the humidity levels\n",
      "4. Check the wind speed and direction\n",
      "5. Check the UV index\n",
      "6. Check the air quality\n",
      "7. Check the sunrise and sunset times\n",
      "8. Check for any weather alerts or warnings\n",
      "9. Write the weather report\n",
      "\u001b[95m\u001b[1m\n",
      "*****TASK LIST*****\n",
      "\u001b[0m\u001b[0m\n",
      "2: Gather relevant data from local weather stations\n",
      "3: Analyze the data to determine the current weather conditions\n",
      "4: Check the current temperature in San Francisco\n",
      "5: Check the forecast for the day\n",
      "6: Check the humidity levels\n",
      "7: Check the wind speed and direction\n",
      "8: Check the UV index\n",
      "9: Check the air quality\n",
      "10: Check for any weather alerts or warnings\n",
      "11: Write the sunrise and sunset times\n",
      "12: Write the weather report\n",
      "\u001b[92m\u001b[1m\n",
      "*****NEXT TASK*****\n",
      "\u001b[0m\u001b[0m\n",
      "2: Gather relevant data from local weather stations\n",
      "\u001b[93m\u001b[1m\n",
      "*****TASK RESULT*****\n",
      "\u001b[0m\u001b[0m\n",
      "\n",
      "\n",
      "I am gathering data from local weather stations in San Francisco to create a weather report for today. I am collecting temperature, humidity, wind speed, and precipitation data from the National Weather Service, as well as any other relevant information.\n",
      "\u001b[95m\u001b[1m\n",
      "*****TASK LIST*****\n",
      "\u001b[0m\u001b[0m\n",
      "3: Check the current temperature in San Francisco\n",
      "4: Check the forecast for the day\n",
      "5: Check the humidity levels\n",
      "6: Check the wind speed and direction\n",
      "7: Check the UV index\n",
      "8: Check the air quality\n",
      "9: Check for any weather alerts or warnings\n",
      "10: Write the sunrise and sunset times\n",
      "11: Compare the current temperature to the average temperature for this time of year.\n",
      "12: Compare the current humidity levels to the average humidity levels for this time of year.\n",
      "13: Compare the current wind speed to the average wind speed for this time of year.\n",
      "14: Compare the current precipitation levels to the average precipitation levels for this time of year.\n",
      "15: Check for any upcoming weather patterns that could affect the forecast.\n",
      "16: Check for any upcoming weather events that could affect the forecast.\n",
      "17: Check for any upcoming weather advisories that could affect the forecast.\n",
      "18: Check for any upcoming weather warnings that could affect the forecast.\n",
      "19: Check for any upcoming weather alerts that could affect the forecast.\n",
      "20: Check for any upcoming weather watches that could affect the forecast.\n",
      "21: Analy\n",
      "\u001b[92m\u001b[1m\n",
      "*****NEXT TASK*****\n",
      "\u001b[0m\u001b[0m\n",
      "3: Check the current temperature in San Francisco\n",
      "\u001b[93m\u001b[1m\n",
      "*****TASK RESULT*****\n",
      "\u001b[0m\u001b[0m\n",
      "\n",
      "\n",
      "I am checking the current temperature in San Francisco. According to the data gathered from local weather stations, the current temperature in San Francisco is 57 degrees Fahrenheit.\n",
      "\u001b[91m\u001b[1m\n",
      "*****TASK ENDING*****\n",
      "\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'objective': 'Write a weather report for SF today'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baby_agi({\"objective\": OBJECTIVE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92d8f1-d254-4c37-8557-819c95761d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
